# Day 1: ML Fundamentals - Question 7
## Debugging Train-Production Performance Gap (No Code Version)

**Question:** Your ML model performs well in training (95% accuracy) but poorly in production (65% accuracy). Walk through your systematic debugging process.

---

## Answer:

This is one of the most common and critical problems in production ML systems. A 30-point gap between training and production is a red flag that requires immediate investigation.

### The Train-Production Performance Gap

This isn't just a technical problem - it's a business problem:
- Wrong business decisions being made
- Customer experience degraded
- Resources wasted on bad predictions
- Trust in ML systems damaged

---

## Phase 1: Quick Triage (First 15 Minutes)

Your first priority is understanding WHAT is happening before investigating WHY.

### Step 1: Confirm the Problem is Real

Don't assume the metrics are correct. Verify:

**Questions to ask:**
- Are we measuring the same thing in training vs production?
- Is production data actually labeled (how do we know it's 65%)?
- Are we comparing apples to apples (same time period, same segments)?
- Could this be a measurement error rather than model error?

**Common false alarms:**
- Training measured on balanced dataset, production on imbalanced data
- Production metrics include edge cases not in training
- Different evaluation periods (training on summer, production in winter)
- Labeling delays (production labels not yet available for recent predictions)

**Action:** Pull exact numbers from both systems and verify methodology.

---

### Step 2: Check Model Deployment

Is the correct model even running?

**Questions to verify:**
- Which model version is deployed?
- When was it deployed?
- Did the deployment succeed?
- Is the model file corrupted?
- Are we loading the correct model weights?

**Common deployment issues:**
- Old model still running (new model never deployed)
- Wrong model version deployed (deployed v1.2 instead of v2.0)
- Model file corrupted during transfer
- Incomplete deployment (rolled out to only some servers)
- Caching issues (serving old predictions)

**How to check:**
- Query model registry for deployed version
- Check deployment logs and timestamps
- Verify model checksums/hashes
- Test prediction on known inputs
- Check if multiple model versions are serving traffic

**If deployment is wrong:** This is an easy fix - deploy the correct model.

---

### Step 3: Quick Metrics Analysis

Look at basic health indicators:

**Key metrics to check immediately:**

**A) Prediction Volume:**
- Expected: 100,000 predictions/day
- Actual: 45,000 predictions/day
- Problem: Why are we getting half the expected traffic?

**B) Error Rate:**
- Predictions failing: 2% (should be <0.1%)
- Timeouts: 5% (should be <0.5%)
- Problem: Model is failing on many inputs

**C) Latency:**
- Expected: 50ms P95
- Actual: 300ms P95
- Problem: Slow predictions might indicate infrastructure issues

**D) Input Data Quality:**
- Missing features: 15% of requests (should be <1%)
- Out-of-range values: 8% of inputs
- Problem: Production data differs from training data

**Action:** These metrics tell you WHERE to investigate deeper.

---

### Step 4: Segment Analysis

Break down the 65% production performance:

**By time:**
- Week 1: 85% (good!)
- Week 2: 75% (declining)
- Week 3: 68% (getting worse)
- Week 4: 65% (current)
- **Pattern:** Continuous degradation = data drift

**By user segment:**
- New users: 50% (bad)
- Power users: 85% (good)
- **Pattern:** Model trained on power users, fails on new users

**By geography:**
- US: 80% (acceptable)
- Europe: 65% (poor)
- Asia: 45% (terrible)
- **Pattern:** Model trained primarily on US data

**By device type:**
- Desktop: 78% (good)
- Mobile: 58% (poor)
- **Pattern:** Different behavior on mobile not captured in training

**Action:** Segmented analysis reveals root causes.

---

## Phase 2: Root Cause Investigation (Next 2 Hours)

Based on Phase 1 findings, investigate the most likely culprits:

---

### Cause 1: Data Drift (Most Common)

**What it means:**
The distribution of input data has changed between training and production. The model learned patterns from one distribution and is now seeing a different distribution.

**How to detect:**

**Compare feature distributions:**

**Training Data (used to train model):**
- Average transaction amount: $47.32
- Standard deviation: $25.18
- 90th percentile: $85.00
- Most common time: 2pm-5pm
- Geographic spread: 60% US, 30% Europe, 10% Asia

**Production Data (current):**
- Average transaction amount: $31.14 (34% lower!)
- Standard deviation: $18.92 (25% lower!)
- 90th percentile: $62.00 (27% lower!)
- Most common time: 7pm-10pm (shifted!)
- Geographic spread: 40% US, 35% Europe, 25% Asia (different!)

**Analysis:**
These are significant shifts. The model learned relationships based on one pattern, but production data follows different patterns.

**Why drift happens:**

**Business reasons:**
- Seasonal changes (trained on summer, now it's winter)
- Marketing campaigns (bringing new user types)
- Product changes (new features, pricing)
- Market shifts (economic conditions, competitor actions)
- Geographic expansion (new markets with different behavior)

**Technical reasons:**
- Upstream data source changed
- Data collection bugs
- Pipeline modifications
- Time-based patterns (weekday vs weekend)

**Real-world example:**

**E-commerce recommendation system:**

**Training data:** January-March 2024
- Users bought winter clothes, heating supplies, indoor entertainment
- Model learned: "Users who buy coats also buy boots"

**Production:** August 2024
- Users buying summer clothes, cooling products, outdoor gear
- Model still recommends: "Buy boots!" with coats
- Customers confused, click-through drops to 65%

**The fix:**
- Retrain model on recent data (June-July)
- Include seasonal features explicitly
- Implement regular retraining schedule
- Monitor distribution shifts continuously

---

### Cause 2: Data Leakage (Model Learned from Future)

**What it means:**
During training, the model had access to information that won't be available at prediction time. It learned to "cheat" using data from the future.

**How to detect:**

**Check feature availability timeline:**

Ask for EVERY feature: "Will this exist when we make predictions?"

**Example: Fraud Detection**

**Training features that might leak:**

**GOOD (Available at prediction time):**
- Transaction amount
- Merchant ID
- User's historical transaction count
- Time of day
- Geographic location
- Device used

**BAD (Leakage - not available yet):**
- "Was this transaction disputed?" (happens days later)
- "User's transaction count next week" (future information)
- "Number of fraud cases reported this day" (includes current transaction)
- Any feature calculated using the target variable

**How leakage manifests:**
- Model shows 95% training accuracy (looks amazing!)
- In production, those leaked features don't exist
- Model makes random guesses without them
- Production accuracy drops to 65%

**Real-world example:**

**Customer churn prediction:**

**Leaked feature (BAD):**
- "Total support tickets in last 30 days" calculated AFTER churn date
- Model learns: "More tickets = churned" (obvious in hindsight!)
- But at prediction time, we're trying to predict churn BEFORE it happens
- We don't have "future" ticket counts
- Model fails

**The fix:**
- Audit every feature for temporal leakage
- Use only point-in-time correct features
- Simulate production conditions during training
- Implement feature validation in pipeline
- Retrain model with clean features

---

### Cause 3: Training-Serving Skew (Different Feature Computation)

**What it means:**
Features are computed differently in training vs production, even though they have the same name.

**How to detect:**

**Sample the same users from training and production:**

**User ID 12345:**

**Training features:**
- Average purchase amount: $67.30
- Purchase frequency: 2.3 per week
- Days since last purchase: 3

**Production features (same user):**
- Average purchase amount: $52.10 (different!)
- Purchase frequency: 1.8 per week (different!)
- Days since last purchase: 5 (different!)

**The problem:** Same user, different feature values. The model is seeing inconsistent inputs.

**Common causes of skew:**

**1. Different Code Paths:**
- Training: Uses Python script with pandas
- Production: Uses Java service with different logic
- Subtle implementation differences cause different results

**2. Different Data Sources:**
- Training: Reads from data warehouse (complete historical data)
- Production: Reads from cache (recent data only)
- Missing historical context changes calculations

**3. Different Time Windows:**
- Training: "Last 30 days" means exactly 30 days historical
- Production: "Last 30 days" might use cached 28-32 day window
- Inconsistent time boundaries

**4. Aggregation Timing:**
- Training: Batch computation at end of day (complete data)
- Production: Real-time computation (incomplete data)
- Different aggregation points

**5. Missing Value Handling:**
- Training: Missing values filled with mean from entire dataset
- Production: Missing values filled with mean from last week only
- Different imputation strategies

**Real-world example:**

**Click-through rate prediction:**

**Training computation:**
- Calculate user's average session length from data warehouse
- Includes all sessions from past year
- Computed in nightly batch job
- Average: 14.3 minutes

**Production computation:**
- Calculate user's average session length from Redis cache
- Only includes sessions from past 7 days (memory constraint)
- Computed in real-time API
- Average: 11.2 minutes (different!)

**Result:**
- Model trained on one value, sees different value in production
- Predictions inconsistent
- Performance degrades

**The fix:**
- Use identical code for training and serving (shared library)
- Use feature store to ensure consistency
- Validate features match before deploying
- Log production features and compare to training
- Containerize feature engineering logic

---

### Cause 4: Label Quality Issues

**What it means:**
The labels used for training were incorrect or different from production ground truth.

**How to detect:**

**Compare labeling methodology:**

**Training labels:**
- Source: Manual review by team A
- Criteria: Subjective judgment
- Time to label: Days after event
- Quality: Variable

**Production labels:**
- Source: Automated from customer actions
- Criteria: Concrete behavior
- Time to label: Immediate
- Quality: Objective but different definition

**Real-world example:**

**Email spam detection:**

**Training labels:**
- Labeled by your team based on "looks spammy"
- Team members have different standards
- Focus on obvious spam
- 10,000 emails labeled

**Production reality:**
- "Spam" is whatever users mark as spam
- User spam definitions vary widely
- Includes things your team wouldn't call spam (newsletters they regret subscribing to)
- Different concept of spam

**Result:**
- Model learns one definition of spam
- Production uses different definition
- Mismatch causes poor performance

**The fix:**
- Use production-like labels for training
- Ensure label consistency
- Document labeling criteria
- Validate inter-rater reliability
- Retrain with production labels

---

### Cause 5: Distribution Shift in Target Variable

**What it means:**
The proportion of positive vs negative cases changed.

**How to detect:**

**Training data:**
- Positive class: 20%
- Negative class: 80%
- Model optimized for this distribution

**Production data:**
- Positive class: 5%
- Negative class: 95%
- Distribution shifted dramatically

**Why this matters:**

Models learn decision boundaries based on class proportions. When proportions change drastically, the learned boundaries are wrong.

**Real-world example:**

**Fraud detection during COVID:**

**Training (2019 data):**
- 0.5% of transactions fraudulent
- Model learned: Be moderately cautious
- Threshold optimized for 0.5% fraud rate

**Production (2020):**
- 3% of transactions fraudulent (6x increase!)
- Economic stress increased fraud
- Model's threshold too lenient
- Missing 80% of fraud cases

**The fix:**
- Monitor class distribution shifts
- Retrain when distribution changes significantly
- Adjust decision thresholds based on current distribution
- Use techniques robust to class imbalance

---

## Phase 3: Immediate Mitigation (While Fixing Root Cause)

You've identified the problem, but fixing it takes time. What do you do NOW?

### Option 1: Rollback to Previous Model

**When to use:**
- Clear degradation after deployment
- Previous model was working well
- New model is significantly worse

**Considerations:**
- Lose any improvements from new model
- Temporary solution only
- Need to fix new model before next deployment

---

### Option 2: Adjust Decision Threshold

**When to use:**
- Model probabilities still good, but threshold wrong
- Class distribution shifted
- Cost-benefit calculation changed

**How it works:**

**Original:**
- Threshold: 0.5 (50% probability = predict positive)
- Optimized for training distribution

**Adjusted:**
- Threshold: 0.3 (30% probability = predict positive)
- Accounts for production distribution shift
- Catches more positives, accepts more false positives

---

### Option 3: Ensemble with Simple Baseline

**When to use:**
- Model works for some cases, fails for others
- Have simple rule-based system as backup

**How it works:**

**For each prediction:**
- Get ML model prediction: 65% confidence fraud
- Get rule-based prediction: Transaction amount > $1000 AND new user = fraud
- Combine: Use rule-based for edge cases, ML for normal cases

**Benefits:**
- Prevents catastrophic failures
- Maintains baseline performance
- Buys time to fix ML model

---

### Option 4: Hybrid Approach with Human Review

**When to use:**
- High-stakes decisions
- Model confidence is low
- Cost of errors is high

**How it works:**

**Route predictions:**
- High confidence (>90%): Auto-approve/reject
- Medium confidence (50-90%): ML prediction with monitoring
- Low confidence (<50%): Human review queue

**Benefits:**
- Prevents worst errors
- Maintains service quality
- Provides feedback for model improvement

---

## Phase 4: Long-Term Prevention

### 1. Continuous Monitoring

**Set up dashboards tracking:**

**Model Performance:**
- Training accuracy vs production accuracy (gap should be <10%)
- Accuracy over time (trend should be stable)
- Accuracy by segment (should be consistent)
- Error types (false positives vs false negatives)

**Data Quality:**
- Feature distributions (compare to training)
- Missing value rates (should be stable)
- Out-of-range values (should be rare)
- Input data volume (should match expectations)

**System Health:**
- Prediction latency (should meet SLA)
- Error rates (should be <1%)
- Prediction volume (should match traffic)
- Model version in production (should be expected version)

**Alerts to configure:**
- Production accuracy drops >5%
- Train-production gap exceeds 15%
- Feature distribution shifts >20%
- Prediction volume changes >50%
- Error rate exceeds 1%

---

### 2. Automated Retraining Pipeline

**Trigger retraining when:**
- Performance drops below threshold
- Data drift detected
- Scheduled interval reached (weekly/monthly)
- New labeled data volume reaches threshold

**Retraining process:**
1. Collect recent labeled data
2. Validate data quality
3. Train new model
4. Evaluate on held-out test set
5. Compare to current production model
6. Deploy if improvement confirmed
7. Monitor deployment

---

### 3. Feature Validation

**Implement checks:**

**Training-time validation:**
- No features derived from target
- All features will be available at prediction time
- Feature distributions make sense
- No extreme outliers or data errors

**Deployment-time validation:**
- Features computed identically to training
- Feature values in expected ranges
- Compare sample of production features to training features
- Reject deployment if validation fails

**Production-time validation:**
- Log features for random sample of predictions
- Compare to training distribution
- Alert if drift detected
- Investigate anomalies

---

### 4. Shadow Deployments

**Before full deployment:**

**Run new model in shadow mode:**
- Production traffic goes to old model (real decisions)
- Same traffic also goes to new model (logged, not used)
- Compare predictions from both models
- Compare performance when labels arrive
- Analyze differences

**Benefits:**
- Catch problems before they impact users
- Understand new model behavior
- Validate improvements are real
- Safe testing environment

**Criteria for promotion:**
- Shadow model outperforms production model
- No unexpected behaviors detected
- Performance consistent across segments
- Stakeholder approval

---

### 5. A/B Testing Framework

**Gradual rollout:**

**Phase 1 (10% traffic):**
- Deploy new model to 10% of users
- Compare metrics to control group (90% on old model)
- Monitor closely
- Duration: 3-7 days

**Phase 2 (50% traffic):**
- If Phase 1 successful, increase to 50%
- Statistical significance more achievable
- Duration: 1-2 weeks

**Phase 3 (100% traffic):**
- If Phase 2 successful, full rollout
- Continue monitoring
- Keep old model as backup

**Rollback criteria:**
- Performance worse than control
- Errors increase
- User complaints increase
- Business metrics degrade

---

## Complete Debugging Checklist

**Deployment Issues:**
- â˜ Correct model version deployed?
- â˜ Model file not corrupted?
- â˜ Deployment completed successfully?
- â˜ Only one model version serving?

**Data Issues:**
- â˜ Input data distribution same as training?
- â˜ Features computed identically?
- â˜ No missing features in production?
- â˜ Data sources unchanged?
- â˜ No pipeline bugs?

**Model Issues:**
- â˜ Model appropriate for production distribution?
- â˜ No data leakage in training?
- â˜ Labels consistent between training and production?
- â˜ Model not stale (trained on recent data)?

**Measurement Issues:**
- â˜ Same metrics in training and production?
- â˜ Labels available for production evaluation?
- â˜ Comparing same time periods?
- â˜ Accounting for class imbalance?

**Infrastructure Issues:**
- â˜ Sufficient resources (memory, CPU)?
- â˜ No timeouts or failures?
- â˜ Network latency acceptable?
- â˜ Dependencies available and correct versions?

---

## Interview Answer Framework

**"When I see a 30-point gap between training (95%) and production (65%), I follow a systematic approach:**

**Immediate (15 minutes):**
- Verify the metrics are real and comparable
- Check which model version is deployed
- Look at basic health indicators (errors, latency, volume)
- Segment performance to identify patterns

**Investigation (2 hours):**
- **Most likely: Data drift** - Compare feature distributions
- **Check for: Training-serving skew** - Validate feature computation
- **Investigate: Data leakage** - Audit features for temporal issues
- **Verify: Label quality** - Ensure consistent labeling

**Mitigation (immediate):**
- Rollback if necessary
- Adjust thresholds if distribution shifted
- Route low-confidence predictions to review
- Combine with baseline system

**Long-term prevention:**
As a data engineer building production ML, I'd implement:
- **Monitoring dashboards** tracking train-prod gap continuously
- **Automated retraining** when performance degrades
- **Feature validation** to ensure training-serving consistency
- **Shadow deployments** before full rollout
- **A/B testing** for gradual, safe deployments

**The key is catching issues before they impact users through comprehensive monitoring and validation infrastructure."**

---

**END OF Q7**

# Day 1: ML Fundamentals - Question 8
## ML Model Monitoring in Production (No Code Version)

**Question:** How would you approach monitoring and maintaining ML models in production? What metrics matter?

---

## Answer:

ML models in production require continuous monitoring and maintenance. Unlike traditional software that degrades only when code breaks, ML models degrade naturally over time as the world changes.

### Why ML Model Monitoring is Different

**Traditional Software:**
- Breaks when: Code has bugs, servers fail, dependencies break
- Behavior: Deterministic - same input always gives same output
- Degradation: Sudden and obvious (crashes, errors)
- Fix: Debug code, fix bug, redeploy

**ML Models:**
- Degrades when: World changes, data drifts, patterns shift
- Behavior: Probabilistic - same input might give different outputs
- Degradation: Gradual and subtle (slow accuracy decline)
- Fix: Retrain with new data, tune parameters, redesign features

**The challenge:** Traditional monitoring tools don't catch ML-specific problems.

---

## The Four Layers of ML Model Monitoring

```
Layer 4: Business Metrics (What the business cares about)
         â†‘
Layer 3: Model Performance (How accurate is the model)
         â†‘
Layer 2: Model Behavior (What is the model predicting)
         â†‘
Layer 1: Infrastructure (Is the system running)
```

Each layer builds on the previous one. All four are necessary.

---

## Layer 1: Infrastructure Monitoring (Foundation)

**What you're monitoring:** Is the system operationally healthy?

This is standard software monitoring, but essential as a foundation.

### Key Metrics

**Availability:**
- System uptime: Should be >99.9%
- Service health checks
- Endpoint response rates
- Database connectivity

**Latency:**
- Prediction latency (P50, P95, P99)
- End-to-end request time
- Feature computation time
- Model inference time

**Throughput:**
- Requests per second
- Predictions per second
- Request queue depth
- Concurrency levels

**Resource Usage:**
- CPU utilization
- Memory consumption
- Disk I/O
- Network bandwidth
- GPU utilization (if applicable)

**Error Rates:**
- HTTP 5xx errors
- Timeouts
- Failed predictions
- Exception rates

### Example Alert Rules

**Critical (Page someone immediately):**
- System down for >5 minutes
- Error rate >5%
- P95 latency >3x normal
- No predictions in last 10 minutes

**Warning (Investigate during business hours):**
- Error rate 1-5%
- P95 latency 2-3x normal
- CPU consistently >80%
- Disk space <20%

### Why This Matters

If your prediction service is down or timing out, it doesn't matter how accurate your model is. Infrastructure problems must be caught first.

**Real-world scenario:**

**E-commerce recommendation system:**
- Infrastructure monitoring shows: P95 latency jumped from 50ms to 400ms
- Users experiencing slow page loads
- Investigation reveals: Feature cache expired, hitting database for every request
- Quick fix: Restore cache, latency returns to 50ms
- Impact prevented: Would have caused user drop-off

---

## Layer 2: Model Behavior Monitoring

**What you're monitoring:** What is the model actually predicting?

This layer catches problems before they manifest as accuracy drops.

### Key Metrics

#### Prediction Distribution

**For classification:**
- Class distribution over time
- Should be relatively stable

**Example - Fraud Detection:**
- Typically predict 0.5% fraud
- If suddenly predicting 5% fraud â†’ investigate
- If suddenly predicting 0.05% fraud â†’ investigate

**Why it matters:** Extreme shifts suggest something changed, even before you have labels to measure accuracy.

**For regression:**
- Average predicted value
- Standard deviation
- Min/max values
- Distribution shape

**Example - House Price Prediction:**
- Typically predict average $400K, range $200K-$800K
- If suddenly predicting average $800K â†’ investigate
- Either data changed or model broken

---

#### Prediction Confidence

**For probabilistic models:**
- Distribution of confidence scores
- Percentage of predictions at extreme confidence (near 0% or 100%)

**Well-calibrated model:**
- Confidence scores spread across range
- Not too many extreme predictions
- When predicts 70%, correct ~70% of the time

**Poorly calibrated model:**
- All predictions at extremes (99% or 1%)
- Over-confident in wrong predictions
- Sign of overfitting

**Example - Medical Diagnosis:**
- Model predicting 95%+ confidence for everything
- Red flag: Overconfident model
- Dangerous: Acting on false confidence
- Need: Recalibration or retraining

---

#### Feature Distribution

**Monitor input features:**
- Mean, median, standard deviation
- Min/max values
- Missing value rates
- Distribution shape

**Compare to training distribution:**
- Should be similar
- Significant shifts indicate data drift

**Example - Customer Churn:**

**Training:**
- Average customer age: 42 years
- Average tenure: 18 months
- 15% missing values

**Production (suddenly):**
- Average customer age: 28 years (shifted!)
- Average tenure: 6 months (shifted!)
- 35% missing values (doubled!)

**Analysis:**
- Clear data shift
- Model trained on different population
- Performance will degrade
- Need: Retrain on current data

---

#### Prediction Consistency

**For same or similar inputs:**
- Predictions should be stable
- Small input changes shouldn't cause large prediction changes

**Example - Loan Approval:**
- Applicant A: $50K income â†’ 65% approval probability
- Applicant B: $51K income â†’ 18% approval probability
- Red flag: Unstable predictions
- Indicates: Model issues or feature computation problems

### Alert Rules for Model Behavior

**Critical:**
- Prediction distribution shifts >30%
- >20% of predictions missing
- All predictions same value
- Feature distributions shift >40%

**Warning:**
- Prediction distribution shifts 15-30%
- Feature distributions shift 20-40%
- Confidence distribution becomes more extreme
- Missing feature values increase >10%

---

## Layer 3: Model Performance Monitoring

**What you're monitoring:** Is the model making accurate predictions?

This is the most important layer, but requires ground truth labels.

### The Challenge: Delayed Labels

Many ML applications have a labeling delay:

**Fraud Detection:**
- Prediction: Instant
- True label: Days or weeks later (investigation completes)
- Delay: Can't measure accuracy immediately

**Customer Churn:**
- Prediction: Today
- True label: 30 days later (did they churn?)
- Delay: Must wait to measure accuracy

**Loan Default:**
- Prediction: At application
- True label: Months or years later
- Delay: Very long feedback loop

### Solutions for Delayed Labels

**1. Proxy Metrics (Immediate):**

Use early indicators that correlate with final outcome:

**Fraud detection proxy:**
- User immediately disputes charge â†’ Likely fraud
- User pays off card â†’ Likely not fraud
- Available immediately, though not perfect

**Churn prediction proxy:**
- User engagement in first week
- Support ticket volume
- Login frequency
- Available quickly, correlates with churn

**2. Sample-Based Evaluation:**

**Randomly sample predictions:**
- Fast-track labeling for sample
- Get quicker feedback
- Estimate overall performance

**Example:**
- Sample 1% of predictions
- Expedite labeling
- Measure accuracy on sample
- Extrapolate to full population

**3. Delayed but Complete Evaluation:**

**Accept the delay:**
- Measure accuracy for predictions from N days ago
- Track trend over time
- Slower feedback but accurate

**Example:**
- Today: Measure accuracy for predictions from 30 days ago
- Tomorrow: Measure accuracy for predictions from 29 days ago
- See trends even with delay

---

### Key Performance Metrics

#### For Classification

**Accuracy:**
- Correct predictions / Total predictions
- Simple but can be misleading with imbalanced data

**Precision:**
- True Positives / (True Positives + False Positives)
- "When we predict positive, how often are we right?"
- Important when false positives are costly

**Recall:**
- True Positives / (True Positives + False Negatives)
- "Of all actual positives, how many did we catch?"
- Important when false negatives are costly

**F1-Score:**
- Harmonic mean of precision and recall
- Balances both concerns

**AUC-ROC:**
- Area under ROC curve
- Threshold-independent metric
- Good for comparing models

**Confusion Matrix:**
- Breakdown of all prediction types
- Shows where model succeeds and fails
- Essential for understanding errors

#### For Regression

**Mean Absolute Error (MAE):**
- Average absolute difference
- Interpretable in original units
- Example: "Predictions off by $5,000 on average"

**Root Mean Squared Error (RMSE):**
- Penalizes large errors more
- Common standard metric

**Mean Absolute Percentage Error (MAPE):**
- Error as percentage
- Good for comparing across scales
- Example: "Predictions off by 12% on average"

**R-Squared:**
- Proportion of variance explained
- Range: 0 to 1 (higher is better)
- Intuitive interpretation

### Alert Rules for Model Performance

**Critical:**
- Accuracy drops >10% from baseline
- Precision or recall drops >15%
- F1-score drops >10%
- Error rate doubles

**Warning:**
- Accuracy drops 5-10% from baseline
- Precision or recall drops 10-15%
- Consistent downward trend over time

---

## Layer 4: Business Metrics Monitoring

**What you're monitoring:** Is the model creating business value?

The ultimate measure of success. A technically accurate model that doesn't improve business outcomes is a failure.

### Key Business Metrics

#### Revenue Impact

**Example - Recommendation System:**
- Conversion rate from recommendations
- Average order value from recommendations
- Revenue attributed to recommendations
- Customer lifetime value

**Target:**
- Conversion rate >3%
- Revenue attribution >$5M/month

#### Cost Savings

**Example - Fraud Detection:**
- Fraud losses prevented
- False positive costs (declined legitimate transactions)
- Investigation costs
- Net savings

**Calculation:**
- Fraud caught: $10M prevented
- False positives: $500K in lost sales
- Investigation costs: $1M
- Net benefit: $8.5M

#### User Experience

**Example - Content Moderation:**
- User-reported content (lower is better)
- Appeal rate on decisions
- Time to decision
- User satisfaction scores

#### Operational Efficiency

**Example - Support Ticket Routing:**
- Time to resolution (should decrease)
- Correct routing rate (should increase)
- Agent satisfaction (should increase)
- Ticket backlog (should decrease)

---

### Real-World Business Monitoring Example

**E-commerce Product Recommendations:**

**Model Performance (Layer 3):**
- Click-through rate prediction accuracy: 85%
- Looks good technically!

**Business Metrics (Layer 4):**
- Conversion rate: 2.1%
- Baseline (no recommendations): 2.0%
- Improvement: Only 0.1 percentage points

**Analysis:**
- Model is accurate at predicting clicks
- But users click and don't buy
- Not driving business value

**Action:**
- Redesign model to optimize for purchases, not clicks
- Add features related to purchase intent
- Change loss function to weight purchases more

**After changes:**
- Click-through prediction accuracy: 78% (went down!)
- Conversion rate: 3.5% (went up significantly!)
- Business value delivered

**Lesson:** Technical metrics don't always align with business value. Monitor both.

---

## Data Drift Monitoring

**What is data drift:**
The distribution of input features changes over time, making the model less accurate.

### Types of Drift

**Covariate Drift:**
- Input distribution X changes
- Relationship Xâ†’Y stays same
- Example: More young users, but young users behave same way

**Prior Probability Drift:**
- Output distribution Y changes
- Example: Fraud rate increases from 0.5% to 2%

**Concept Drift:**
- Relationship Xâ†’Y changes
- Example: Features that predicted churn no longer do
- Most problematic type

### How to Detect Drift

#### Statistical Tests

**Kolmogorov-Smirnov (KS) Test:**
- Compares two distributions
- Tests if they're significantly different
- Apply to each feature

**Process:**
- Compare current production feature distribution
- To training feature distribution
- P-value < 0.05 indicates significant drift

**Population Stability Index (PSI):**
- Measures distribution change
- **PSI < 0.1:** No significant change
- **PSI 0.1-0.25:** Some change, monitor
- **PSI > 0.25:** Significant drift, retrain

#### Visual Inspection

- Plot feature distributions over time
- Look for obvious shifts
- Compare to training distribution

**Example - Customer Age Distribution:**

**Training (2023):**
- Age distribution: Peak at 45, normal distribution

**Production (2024):**
- Age distribution: Peak at 30, shifted left
- Clear drift visible in histogram

### Alert Rules for Data Drift

**Critical:**
- PSI > 0.25 for any important feature
- >30% of features show significant drift (p < 0.01)
- Core features shift dramatically

**Warning:**
- PSI 0.1-0.25 for important features
- 15-30% of features show drift
- Gradual drift over time

---

## Model Retraining Strategy

### When to Retrain

**Trigger 1: Performance Degradation**
- Accuracy drops below threshold
- Business metrics decline
- Immediate action needed

**Trigger 2: Data Drift**
- Significant feature distribution shifts
- PSI > 0.25
- Proactive retraining

**Trigger 3: Scheduled**
- Regular cadence (weekly, monthly)
- Prevents gradual degradation
- Even if metrics stable

**Trigger 4: Data Volume**
- Accumulated N new labeled examples
- Enough new data to improve model
- Example: Retrain after 100K new labels

**Trigger 5: Business Event**
- Major product change
- Market shift
- New regulations
- Seasonal change

### Retraining Process

**Step 1: Collect Fresh Data**
- Recent labeled examples
- Representative of current patterns
- Quality validated

**Step 2: Train New Model**
- Same architecture or experiment with new
- Tune hyperparameters
- Use cross-validation

**Step 3: Offline Evaluation**
- Test on held-out recent data
- Compare to current production model
- Must beat current model to deploy

**Step 4: Shadow Deployment**
- Run in parallel with production model
- Don't make real decisions yet
- Compare predictions and performance

**Step 5: A/B Test**
- Gradual rollout (10% â†’ 50% â†’ 100%)
- Monitor business and technical metrics
- Rollback if issues detected

**Step 6: Full Deployment**
- Replace production model
- Archive old model (for potential rollback)
- Update documentation

**Step 7: Continue Monitoring**
- New model becomes baseline
- Continue monitoring all layers
- Cycle repeats

---

## Dashboard Design

### Executive Dashboard (For Stakeholders)

**Content:**
- Business metrics (revenue, costs, satisfaction)
- High-level performance (accuracy trend)
- Model ROI
- Simple visualizations
- Updated daily

**Example:**
- "Fraud Detection Model ROI: $8.5M net benefit this month"
- "Customer Churn Prediction: 92% accuracy, preventing $2M monthly losses"
- Clean charts showing trends

---

### Operations Dashboard (For ML Engineers)

**Content:**
- Layer 1: Infrastructure health
- Layer 2: Model behavior
- Layer 3: Performance metrics
- Layer 4: Business impact
- Data drift indicators
- Updated hourly

**Example sections:**
- System health: Latency, errors, throughput
- Prediction distribution over time
- Accuracy by segment
- Feature drift heatmap
- Business KPIs

---

### Debugging Dashboard (For Troubleshooting)

**Content:**
- Detailed error analysis
- Feature distributions
- Prediction examples (good and bad)
- Confusion matrices
- Segmented performance
- Updated real-time

**Example:**
- Drill down into specific prediction failures
- Compare feature values for correct vs incorrect predictions
- Identify patterns in errors
- Debug specific issues

---

## Monitoring Infrastructure Components

### Data Collection

**What to log:**
- All predictions with timestamps
- Input features (sample or all)
- Model version used
- Prediction confidence
- Latency metrics
- Errors and exceptions

**Storage:**
- Hot storage: Last 30 days in database
- Warm storage: Last year in data warehouse
- Cold storage: Historical in S3/archive

### Metric Computation

**Real-time metrics:**
- Compute from streaming data
- Update every few seconds
- Infrastructure and behavior metrics

**Batch metrics:**
- Compute from stored data
- Update hourly or daily
- Performance and business metrics

### Alerting System

**Alert routing:**
- Critical: Page on-call engineer
- High: Slack channel + email
- Medium: Email only
- Low: Dashboard notification

**Alert fatigue prevention:**
- Use appropriate thresholds
- Aggregate similar alerts
- Provide context and runbooks
- Track alert accuracy

---

## Monitoring Best Practices

### 1. Start Simple, Add Complexity

**Initial setup:**
- Basic infrastructure monitoring
- Prediction distribution tracking
- Manual performance checks

**Iterate:**
- Add drift detection
- Automate performance evaluation
- Build custom dashboards
- Implement automated retraining

### 2. Monitor Leading Indicators

**Don't wait for accuracy to drop:**
- Track data drift (predicts problems)
- Monitor prediction behavior
- Watch for anomalies
- Catch issues early

### 3. Make Monitoring Actionable

**Every alert should have:**
- Clear description of issue
- Impact assessment
- Runbook for response
- Escalation path

**Example alert:**
```
CRITICAL: Fraud model data drift detected
- PSI: 0.32 (threshold: 0.25)
- Affected features: transaction_amount, merchant_category
- Impact: Model accuracy likely degrading
- Action: Review runbook at [link]
- Escalate to: ML team lead if not resolved in 2 hours
```

### 4. Balance Automation and Human Review

**Automate:**
- Data collection
- Metric computation
- Alert generation
- Routine retraining

**Keep human in loop for:**
- Investigating anomalies
- Deciding on major changes
- Approving deployments
- Strategic decisions

### 5. Document Everything

**Maintain documentation for:**
- What each metric means
- Normal ranges for metrics
- Alert thresholds and rationale
- Incident response procedures
- Model versions and changes

---

## Common Monitoring Mistakes

### Mistake 1: Only Monitoring Accuracy

**Problem:** Accuracy is lagging indicator
**Solution:** Monitor behavior and drift first

### Mistake 2: No Business Metrics

**Problem:** Technical success â‰  business value
**Solution:** Always connect to business outcomes

### Mistake 3: Alert Fatigue

**Problem:** Too many alerts, team ignores them
**Solution:** Tune thresholds, reduce noise

### Mistake 4: Monitoring Without Action

**Problem:** Collect metrics but don't use them
**Solution:** Every dashboard needs an owner and action plan

### Mistake 5: Assuming Stability

**Problem:** "Model works, no need to monitor"
**Solution:** World changes, models degrade naturally

---

## Interview Answer Framework

**"For production ML monitoring, I think in four layers:**

**Layer 1 - Infrastructure:**
- Standard metrics: latency, errors, throughput
- Ensures system is operational
- Alerts: Downtime, extreme latency, error spikes

**Layer 2 - Model Behavior:**
- Prediction distributions
- Feature distributions
- Confidence calibration
- Catches issues before accuracy degrades
- Alerts: Distribution shifts, extreme predictions

**Layer 3 - Model Performance:**
- Accuracy, precision, recall for classification
- MAE, RMSE for regression
- Requires ground truth labels (may be delayed)
- Alerts: Performance drops below threshold

**Layer 4 - Business Impact:**
- Revenue, costs, user satisfaction
- Ultimate measure of success
- Alerts: Business KPIs decline

**As a data engineer, I'd also implement:**
- **Data drift monitoring** with statistical tests (PSI, KS test)
- **Automated retraining** when drift or degradation detected
- **A/B testing framework** for safe deployments
- **Comprehensive dashboards** for different audiences
- **Alert hierarchies** (critical vs warning)

**Key principle:** Don't wait for accuracy to drop. Monitor leading indicators (data drift, prediction behavior) to catch problems early and retrain proactively.

**Retraining triggers:**
- Performance drops >5%
- Data drift PSI >0.25
- Scheduled (monthly minimum)
- After major business changes

**The goal is zero-surprise production ML** - catch and fix issues before they impact users. This requires monitoring at all four layers and acting on early warning signs."

---

## Real-World Monitoring Example

**Scenario: E-commerce Search Ranking**

**Layer 1 - Infrastructure:**
- Latency: P95 = 45ms (target: <50ms) âœ“
- Error rate: 0.05% (target: <0.1%) âœ“
- Throughput: 5,000 QPS âœ“

**Layer 2 - Behavior:**
- Average predicted relevance score: 0.65 (was 0.68 last week) âš ï¸
- 15% of predictions have confidence <0.3 (was 8%) âš ï¸
- Feature "user_search_history_length" shifted (mean: 8 â†’ 12) âš ï¸

**Layer 3 - Performance:**
- NDCG@10: 0.72 (was 0.75 two weeks ago) âš ï¸
- Click-through rate on top result: 28% (was 32%) âš ï¸
- Mean reciprocal rank: 0.68 (was 0.71) âš ï¸

**Layer 4 - Business:**
- Conversion rate: 2.8% (was 3.1%) ðŸš¨
- Revenue per search: $4.20 (was $4.80) ðŸš¨
- User satisfaction score: 4.1/5 (was 4.3/5) ðŸš¨

**Analysis:**
- Clear degradation across all layers
- Started with behavior changes (Layer 2)
- Led to performance drop (Layer 3)
- Now impacting business (Layer 4)

**Root Cause Investigation:**
- User behavior changed (more mobile searches)
- Model trained primarily on desktop
- Features optimized for desktop patterns

**Action:**
- Immediate: Adjust model thresholds for mobile
- Short-term: Retrain with recent mobile-heavy data
- Long-term: Separate models for mobile/desktop

**Lesson:** Multi-layer monitoring caught issue progression. Could have acted earlier at Layer 2.

---
# Day 1: ML Fundamentals - Question 9
## Real-Time Fraud Detection System Design (No Code Version)

**Question:** Design a real-time fraud detection system that processes 10,000 transactions per second with sub-100ms latency. Walk me through your architecture.

---

## Answer:

This is a classic ML system design question that tests architectural thinking, scaling knowledge, and trade-off analysis.

---

## Step 1: Requirements Clarification (Critical First Step)

Never jump straight to design. Always clarify requirements first.

### Functional Requirements

**1. Scale:**
- 10,000 transactions/second = 864 million/day
- Any peak periods? (Black Friday might be 50K TPS)
- Global or regional? (Affects latency requirements)
- Growth expectations? (Plan for 2-5x growth)

**2. Latency:**
- Sub-100ms for what percentile? (P95? P99?)
- Does this include network time?
- Can we have different latency tiers? (High-risk vs low-risk)

**3. Accuracy:**
- What's acceptable fraud catch rate? (Target: 95%+)
- What's acceptable false positive rate? (Target: <1%)
- Cost of missing fraud vs declining legitimate transactions?

**4. Transaction Types:**
- Credit cards, bank transfers, or both?
- Different patterns for different types?
- International transactions included?

### Non-Functional Requirements

**1. Availability:**
- Target: 99.9% uptime (8.76 hours downtime/year max)
- Can tolerate brief outages? (Probably not - money at stake)
- Need: Geographic redundancy

**2. Data Retention:**
- Transaction data: 7 years (regulatory requirement)
- Features: 90 days (for retraining)
- Model predictions: 1 year (for audit)

**3. Compliance:**
- PCI-DSS for payment data
- GDPR for EU customers
- SOX for financial controls
- Data encryption requirements

**4. Cost:**
- Infrastructure budget?
- Preference for cloud vs on-premise?
- Cost per transaction limit?

---

## Step 2: Back-of-Envelope Calculations

Let's estimate system scale:

### Traffic
- **10,000 TPS average**
- **Peak: 50,000 TPS** (5x average for events like Black Friday)
- **Daily: 864 million transactions**
- **Monthly: 25.9 billion transactions**

### Data Volume
- **Per transaction: ~1KB** (features, metadata)
- **Daily: 864M Ã— 1KB = 864 GB/day**
- **Monthly: ~26 TB/month**
- **Annual: ~315 TB/year**

### Storage Needs
- **Hot storage (90 days): 78 TB**
- **Warm storage (1 year): 315 TB**
- **Cold storage (7 years): 2.2 PB**

### Compute Needs
- **10,000 TPS with 100ms latency = 1,000 concurrent requests**
- **CPU per request: ~10ms**
- **Total CPU: 100 cores minimum**
- **With overhead and peak: 500 cores recommended**

### Feature Store
- **User features:** 100M users Ã— 50 features Ã— 8 bytes = 40 GB
- **Merchant features:** 1M merchants Ã— 30 features Ã— 8 bytes = 240 MB
- **Aggregate features:** 100M users Ã— 20 aggregates Ã— 8 bytes = 16 GB
- **Total: ~60 GB** (easily fits in memory with Redis)

### Model Size
- **XGBoost model: ~100 MB**
- Can load in memory on each server
- Fast inference (~5ms)

### Infrastructure Estimate
- **Prediction servers:** 20-50 instances (with auto-scaling)
- **Feature store:** 3-5 Redis instances (clustered)
- **Database:** Postgres or similar for transactions
- **Model training:** Spark cluster (batch, not real-time critical)
- **Total cloud cost: ~$50K-100K/month**

---

## Step 3: High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLIENT APPLICATIONS                          â”‚
â”‚                    (Web, Mobile, Payment Terminals)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API GATEWAY / LOAD BALANCER                     â”‚
â”‚  â€¢ SSL Termination  â€¢ Rate Limiting  â€¢ Request Routing  â€¢ Auth      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REAL-TIME PREDICTION SERVICE                      â”‚
â”‚                        (Stateless Microservices)                     â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Rules Engine â”‚â†’ â”‚ Feature Store â”‚â†’ â”‚ ML Predictor â”‚            â”‚
â”‚  â”‚   (5ms)      â”‚  â”‚    (30ms)     â”‚  â”‚    (20ms)    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                       â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                            â”‚ Decision Engine â”‚                       â”‚
â”‚                            â”‚     (5ms)       â”‚                       â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DECISION ROUTING                                â”‚
â”‚                                                                       â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚   APPROVE   â”‚     â”‚   REVIEW    â”‚     â”‚   DECLINE    â”‚       â”‚
â”‚    â”‚    (65%)    â”‚     â”‚    (30%)    â”‚     â”‚     (5%)     â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SUPPORTING SYSTEMS                           â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Feature Store  â”‚  â”‚ Batch Training  â”‚  â”‚   Monitoring &   â”‚   â”‚
â”‚  â”‚                 â”‚  â”‚   Pipeline      â”‚  â”‚    Alerting      â”‚   â”‚
â”‚  â”‚  â€¢ Redis (Hot)  â”‚  â”‚  â€¢ Spark Jobs   â”‚  â”‚  â€¢ Dashboards    â”‚   â”‚
â”‚  â”‚  â€¢ S3 (Cold)    â”‚  â”‚  â€¢ Daily @2AM   â”‚  â”‚  â€¢ Real-time     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components Overview

**1. API Gateway / Load Balancer**
- Entry point for transaction requests
- SSL termination
- Rate limiting
- Request routing
- Health checks

**2. Real-Time Prediction Service**
- Stateless microservices
- Horizontal scaling
- Fast inference (<50ms)
- Caching for common patterns

**3. Feature Store**
- Online layer (Redis) - Real-time serving
- Offline layer (S3/Data Lake) - Training
- Feature computation engine
- Feature validation

**4. Rules Engine**
- Quick filtering (5ms)
- Simple boolean rules
- Handles obvious cases
- Reduces ML workload

**5. Decision Engine**
- Combines ML + rules
- Risk scoring
- Threshold management
- Action routing (approve/decline/review)

**6. Batch Processing Pipeline**
- Nightly feature computation
- Model training
- Performance analysis
- Drift detection

**7. Monitoring & Alerting**
- Real-time dashboards
- Performance tracking
- Drift detection
- Alert management

---

## Step 4: Detailed Component Design

### Component 1: Real-Time Prediction Flow

```
INCOMING TRANSACTION
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: RULES ENGINE (5ms)             â”‚
â”‚                                          â”‚
â”‚  Quick Checks:                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Hard Rules (Auto-Decline):      â”‚   â”‚
â”‚  â”‚ â€¢ Known fraudster list          â”‚   â”‚
â”‚  â”‚ â€¢ Amount exceeds limit by 10x   â”‚   â”‚
â”‚  â”‚ â€¢ Blocked country               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Soft Rules (Auto-Approve):      â”‚   â”‚
â”‚  â”‚ â€¢ Small amount (<$10)           â”‚   â”‚
â”‚  â”‚ â€¢ Repeat purchase               â”‚   â”‚
â”‚  â”‚ â€¢ Perfect user history          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                          â”‚
â”‚  Bypass: 40-50% handled here            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼ (If rules don't decide)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: FEATURE EXTRACTION (30ms)      â”‚
â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Transaction Features (5ms)     â”‚    â”‚
â”‚  â”‚ â€¢ Amount, merchant, location   â”‚    â”‚
â”‚  â”‚ â€¢ Time, device, IP             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ User Profile (10ms - Redis)    â”‚    â”‚
â”‚  â”‚ â€¢ Account age, type            â”‚    â”‚
â”‚  â”‚ â€¢ KYC status, risk score       â”‚    â”‚
â”‚  â”‚ â€¢ Home location                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Aggregates (15ms - Redis)      â”‚    â”‚
â”‚  â”‚ â€¢ Transactions last 24h        â”‚    â”‚
â”‚  â”‚ â€¢ Total spend last 24h         â”‚    â”‚
â”‚  â”‚ â€¢ Velocity metrics             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                          â”‚
â”‚  Parallel fetches + caching             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: ML PREDICTION (20ms)           â”‚
â”‚                                          â”‚
â”‚  Model: XGBoost (Gradient Boosted Trees)â”‚
â”‚                                          â”‚
â”‚  Input: 50 features                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ Transaction: 10 features     â”‚    â”‚
â”‚  â”‚ â€¢ User profile: 15 features    â”‚    â”‚
â”‚  â”‚ â€¢ Aggregates: 20 features      â”‚    â”‚
â”‚  â”‚ â€¢ Derived: 5 features          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                          â”‚
â”‚  Output:                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ Fraud probability: 0.0-1.0   â”‚    â”‚
â”‚  â”‚ â€¢ Feature contributions (SHAP) â”‚    â”‚
â”‚  â”‚ â€¢ Prediction confidence        â”‚    â”‚
â”‚  â”‚ â€¢ Model version                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                          â”‚
â”‚  In-memory serving, no network calls    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: DECISION LOGIC (5ms)           â”‚
â”‚                                          â”‚
â”‚  Risk Score Calculation:                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ ML fraud probability: 0.72   â”‚    â”‚
â”‚  â”‚ â€¢ Rules engine score: 0.60     â”‚    â”‚
â”‚  â”‚ â€¢ Historical behavior: 0.45    â”‚    â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚    â”‚
â”‚  â”‚ â€¢ Weighted combination: 0.68   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                          â”‚
â”‚  Thresholds:                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Score < 0.3  â†’ APPROVE         â”‚    â”‚
â”‚  â”‚ Score 0.3-0.7 â†’ REVIEW         â”‚    â”‚
â”‚  â”‚ Score > 0.7  â†’ DECLINE         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                          â”‚
â”‚  Context-aware adjustments              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚   DECISION   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚        â”‚
    â–¼              â–¼        â–¼
 APPROVE        REVIEW   DECLINE
  (65%)         (30%)     (5%)
```

**Total Latency Budget:**
- Rules Engine: 5ms
- Feature Extraction: 30ms
- ML Prediction: 20ms
- Decision Logic: 5ms
- **Total: 60ms (P50), 90ms (P95)** âœ“ Within 100ms requirement

---

### Why Rules Engine First?

**Benefits:**
1. **Handles 40-50% of transactions** without ML computation
2. **Saves computational resources** for complex cases
3. **Reduces latency** for obvious decisions
4. **Clear business logic** that's easy to understand and audit
5. **Fast to update** without model retraining

**Example Rules:**

**Hard Rules (Auto-Decline):**
```
IF user_id IN known_fraudster_list
   THEN DECLINE (confidence: 100%)

IF transaction_amount > (user_avg_amount Ã— 10)
   THEN DECLINE (confidence: 95%)

IF country IN blocked_country_list
   THEN DECLINE (confidence: 100%)
```

**Soft Rules (Auto-Approve):**
```
IF transaction_amount < $10
   AND merchant_category = "coffee_shop"
   AND user_history = "perfect"
   THEN APPROVE (confidence: 98%)

IF transaction_amount SIMILAR_TO last_3_transactions
   AND same_merchant AS last_purchase
   AND time_since_last < 1_hour
   THEN APPROVE (confidence: 95%)
```

---

### Component 2: Feature Store Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FEATURE STORE (TWO-TIER)                     â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚               ONLINE TIER (Real-Time Serving)               â”‚  â”‚
â”‚  â”‚                                                              â”‚  â”‚
â”‚  â”‚  Technology: Redis Cluster (5 nodes, sharded by user_id)   â”‚  â”‚
â”‚  â”‚                                                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”‚
â”‚  â”‚  â”‚   Master 1   â”‚  â”‚   Master 2   â”‚  â”‚   Master 3   â”‚     â”‚  â”‚
â”‚  â”‚  â”‚ (Users A-D)  â”‚  â”‚ (Users E-M)  â”‚  â”‚ (Users N-Z)  â”‚     â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚
â”‚  â”‚         â”‚                  â”‚                  â”‚              â”‚  â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”        â”‚  â”‚
â”‚  â”‚    â”‚Replica 1â”‚        â”‚Replica 2â”‚        â”‚Replica 3â”‚        â”‚  â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚
â”‚  â”‚                                                              â”‚  â”‚
â”‚  â”‚  Data Stored:                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ User profiles: 100M Ã— 2KB = 200GB               â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Aggregate features (24h, 7d, 30d windows)       â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Merchant metadata: 1M Ã— 1KB = 1GB               â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Real-time velocity counters                      â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Total: ~351GB in memory                          â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                                                              â”‚  â”‚
â”‚  â”‚  Performance:                                               â”‚  â”‚
â”‚  â”‚  â€¢ Latency: <10ms P95                                      â”‚  â”‚
â”‚  â”‚  â€¢ Throughput: 100K+ ops/sec per node                     â”‚  â”‚
â”‚  â”‚  â€¢ Read from replicas (load distribution)                  â”‚  â”‚
â”‚  â”‚  â€¢ Write to master (consistency)                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                    â”‚
â”‚                              â–²  â”‚                                  â”‚
â”‚                              â”‚  â–¼                                  â”‚
â”‚                         Sync Nightly                               â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              OFFLINE TIER (Batch Processing)                â”‚  â”‚
â”‚  â”‚                                                              â”‚  â”‚
â”‚  â”‚  Technology: S3 Data Lake + Spark                           â”‚  â”‚
â”‚  â”‚                                                              â”‚  â”‚
â”‚  â”‚  Storage:                                                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚ s3://fraud-detection/                              â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ â”œâ”€â”€ features/                                       â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ â”‚   â”œâ”€â”€ user_profiles/                             â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ â”‚   â”œâ”€â”€ aggregates/                                â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ â”‚   â””â”€â”€ training_sets/                             â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ â”œâ”€â”€ transactions/                                   â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ â”‚   â””â”€â”€ date=2024-12-12/                           â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ â””â”€â”€ labels/                                         â”‚    â”‚  â”‚
â”‚  â”‚  â”‚     â””â”€â”€ fraud_investigations/                       â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                                                              â”‚  â”‚
â”‚  â”‚  Purpose:                                                    â”‚  â”‚
â”‚  â”‚  â€¢ Training data storage (7 years)                          â”‚  â”‚
â”‚  â”‚  â€¢ Historical feature computation                           â”‚  â”‚
â”‚  â”‚  â€¢ Batch feature engineering                                â”‚  â”‚
â”‚  â”‚  â€¢ Model artifacts and versions                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Feature Freshness Strategy

**Real-time Updates (Per Transaction):**
```
When transaction processes:
  INCREMENT user_transaction_count_24h
  ADD transaction_amount TO user_total_spend_24h
  ADD merchant_id TO user_recent_merchants_set
  UPDATE user_last_transaction_timestamp
  
  All operations atomic in Redis
```

**Batch Updates (Nightly @ 2 AM):**
```
Complex features requiring full history:
  â€¢ User behavior patterns (30-90 day analysis)
  â€¢ Merchant risk scores (global statistics)
  â€¢ Baseline user profiles
  â€¢ Long-term aggregates
  
  Computed in Spark, pushed to Redis
```

---

### Component 3: ML Model - XGBoost

**Why XGBoost (Gradient Boosted Trees)?**

**Advantages:**
1. **Fast inference:** 5-20ms for 50 features
2. **Interpretable:** Feature importance, SHAP values
3. **Handles mixed data:** Numerical + categorical features
4. **Robust to missing values:** Built-in handling
5. **Proven in production:** Used by many companies for fraud detection
6. **No preprocessing needed:** Scales, normalization not required

**Alternatives Considered:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Type      â”‚ Inference    â”‚ Accuracy   â”‚ Interpret   â”‚
â”‚                 â”‚ Latency      â”‚            â”‚ ability     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Logistic        â”‚ 2ms          â”‚ Medium     â”‚ High        â”‚
â”‚ Regression      â”‚ âœ“âœ“âœ“          â”‚ âœ—          â”‚ âœ“âœ“âœ“         â”‚
â”‚                 â”‚              â”‚            â”‚             â”‚
â”‚ XGBoost         â”‚ 20ms         â”‚ High       â”‚ High        â”‚
â”‚ (CHOSEN)        â”‚ âœ“âœ“           â”‚ âœ“âœ“âœ“        â”‚ âœ“âœ“âœ“         â”‚
â”‚                 â”‚              â”‚            â”‚             â”‚
â”‚ Deep Neural     â”‚ 50-100ms     â”‚ High       â”‚ Low         â”‚
â”‚ Network         â”‚ âœ—            â”‚ âœ“âœ“âœ“        â”‚ âœ—           â”‚
â”‚                 â”‚              â”‚            â”‚             â”‚
â”‚ Random Forest   â”‚ 30ms         â”‚ Medium-    â”‚ Medium      â”‚
â”‚                 â”‚ âœ“            â”‚ High âœ“âœ“    â”‚ âœ“âœ“          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Decision: XGBoost wins on balance of speed, accuracy, interpretability
```

**Model Serving Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PREDICTION SERVICE (Stateless)                 â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Instance 1         Instance 2         Instance N    â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ XGBoost  â”‚       â”‚ XGBoost  â”‚      â”‚ XGBoost  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ Model    â”‚       â”‚ Model    â”‚      â”‚ Model    â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ (100MB)  â”‚       â”‚ (100MB)  â”‚      â”‚ (100MB)  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚          â”‚       â”‚          â”‚      â”‚          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ Loaded   â”‚       â”‚ Loaded   â”‚      â”‚ Loaded   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ in RAM   â”‚       â”‚ in RAM   â”‚      â”‚ in RAM   â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  Each instance:                                       â”‚  â”‚
â”‚  â”‚  â€¢ Loads model at startup                            â”‚  â”‚
â”‚  â”‚  â€¢ No network calls for inference                    â”‚  â”‚
â”‚  â”‚  â€¢ CPU-based (no GPU needed)                         â”‚  â”‚
â”‚  â”‚  â€¢ 20ms inference time                               â”‚  â”‚
â”‚  â”‚  â€¢ Can serve ~50 requests/sec                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  Auto-scaling:                                              â”‚
â”‚  â€¢ Min instances: 20 (handles 1,000 TPS)                   â”‚
â”‚  â€¢ Max instances: 100 (handles 5,000 TPS)                  â”‚
â”‚  â€¢ Scale up: CPU > 70% for 2 minutes                       â”‚
â”‚  â€¢ Scale down: CPU < 30% for 10 minutes                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Model Input Features (50 total):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEATURE CATEGORIES                        â”‚
â”‚                                                               â”‚
â”‚  1. TRANSACTION FEATURES (10)                                â”‚
â”‚     â€¢ amount                                                  â”‚
â”‚     â€¢ merchant_id, merchant_category                         â”‚
â”‚     â€¢ transaction_time (hour, day_of_week)                   â”‚
â”‚     â€¢ location (lat, long, country)                          â”‚
â”‚     â€¢ device_type, ip_address                                â”‚
â”‚     â€¢ international (boolean)                                 â”‚
â”‚                                                               â”‚
â”‚  2. USER PROFILE FEATURES (15)                               â”‚
â”‚     â€¢ account_age_days                                        â”‚
â”‚     â€¢ account_type (personal, business)                      â”‚
â”‚     â€¢ kyc_verification_status                                â”‚
â”‚     â€¢ historical_risk_score                                   â”‚
â”‚     â€¢ home_location (lat, long)                              â”‚
â”‚     â€¢ avg_transaction_amount                                  â”‚
â”‚     â€¢ total_lifetime_transactions                            â”‚
â”‚     â€¢ previous_fraud_reports                                 â”‚
â”‚     â€¢ account_balance                                         â”‚
â”‚                                                               â”‚
â”‚  3. AGGREGATE FEATURES (20)                                  â”‚
â”‚     Time windows: 1h, 24h, 7d, 30d                          â”‚
â”‚     â€¢ transaction_count_[window]                             â”‚
â”‚     â€¢ total_spend_[window]                                   â”‚
â”‚     â€¢ distinct_merchants_[window]                            â”‚
â”‚     â€¢ distinct_countries_[window]                            â”‚
â”‚     â€¢ declined_transactions_[window]                         â”‚
â”‚     â€¢ device_changes_[window]                                â”‚
â”‚     â€¢ max_transaction_amount_[window]                        â”‚
â”‚                                                               â”‚
â”‚  4. DERIVED FEATURES (5)                                     â”‚
â”‚     â€¢ amount_deviation_from_avg (z-score)                    â”‚
â”‚     â€¢ time_since_last_transaction                            â”‚
â”‚     â€¢ distance_from_home_location                            â”‚
â”‚     â€¢ merchant_risk_score (pre-computed)                     â”‚
â”‚     â€¢ time_of_day_risk (unusual hour?)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Model Output:**

```
{
  "fraud_probability": 0.72,
  "confidence": 0.85,
  "model_version": "v2024-12-11",
  "feature_contributions": {
    "amount_deviation_from_avg": +0.15,
    "distance_from_home": +0.12,
    "transaction_count_24h": +0.08,
    "merchant_risk_score": +0.05,
    ...
  },
  "prediction_timestamp": "2024-12-12T10:30:45Z",
  "latency_ms": 18
}
```

---

### Component 4: Batch Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           NIGHTLY TRAINING PIPELINE (Runs @ 2 AM)            â”‚
â”‚                                                                â”‚
â”‚  STEP 1: DATA COLLECTION (30 min)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Collect transactions from 7-90 days ago                â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Why this window?                                        â”‚  â”‚
â”‚  â”‚ â€¢ 7 days: Allows fraud investigations to complete      â”‚  â”‚
â”‚  â”‚ â€¢ 90 days: Recent patterns, not stale                  â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Data sources:                                           â”‚  â”‚
â”‚  â”‚ â€¢ S3: Transaction history                              â”‚  â”‚
â”‚  â”‚ â€¢ S3: Fraud labels (investigations complete)           â”‚  â”‚
â”‚  â”‚ â€¢ Redis: Feature snapshots                             â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Result: ~10M labeled transactions                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â”‚  STEP 2: FEATURE ENGINEERING (45 min)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Compute same features as real-time                     â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Critical: Training-serving consistency                  â”‚  â”‚
â”‚  â”‚ â€¢ Use same code as production                          â”‚  â”‚
â”‚  â”‚ â€¢ Shared feature computation library                   â”‚  â”‚
â”‚  â”‚ â€¢ Validate distributions match                          â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Feature validation:                                     â”‚  â”‚
â”‚  â”‚ â€¢ Check for data leakage (future info)                 â”‚  â”‚
â”‚  â”‚ â€¢ Verify point-in-time correctness                     â”‚  â”‚
â”‚  â”‚ â€¢ Confirm all features available at prediction time    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â”‚  STEP 3: HANDLE CLASS IMBALANCE (15 min)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Problem: 0.5% fraud rate (highly imbalanced)           â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Original:                                               â”‚  â”‚
â”‚  â”‚ â€¢ Legitimate: 9,950,000 transactions                   â”‚  â”‚
â”‚  â”‚ â€¢ Fraud: 50,000 transactions                           â”‚  â”‚
â”‚  â”‚ â€¢ Ratio: 199:1                                         â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Solution: Undersample majority class                    â”‚  â”‚
â”‚  â”‚ â€¢ Keep all fraud: 50,000                               â”‚  â”‚
â”‚  â”‚ â€¢ Sample legitimate: 250,000 (random)                  â”‚  â”‚
â”‚  â”‚ â€¢ New ratio: 5:1 (more balanced)                       â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Final training set: 300,000 transactions               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â”‚  STEP 4: MODEL TRAINING (60 min)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Algorithm: XGBoost                                      â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Key parameters:                                         â”‚  â”‚
â”‚  â”‚ â€¢ objective: binary classification                      â”‚  â”‚
â”‚  â”‚ â€¢ eval_metric: AUC, PR-AUC                             â”‚  â”‚
â”‚  â”‚ â€¢ max_depth: 6 (controls complexity)                   â”‚  â”‚
â”‚  â”‚ â€¢ learning_rate: 0.1                                   â”‚  â”‚
â”‚  â”‚ â€¢ num_trees: 100                                       â”‚  â”‚
â”‚  â”‚ â€¢ scale_pos_weight: 5 (handles imbalance)             â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Cross-validation: 5-fold                               â”‚  â”‚
â”‚  â”‚ Early stopping: 10 rounds                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â”‚  STEP 5: MODEL EVALUATION (30 min)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Hold-out test set: Last 7 days (labeled)              â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Metrics:                                                â”‚  â”‚
â”‚  â”‚ â€¢ AUC: 0.94 (excellent discrimination)                 â”‚  â”‚
â”‚  â”‚ â€¢ Precision: 0.85 (85% flagged are fraud)             â”‚  â”‚
â”‚  â”‚ â€¢ Recall: 0.92 (catch 92% of fraud)                   â”‚  â”‚
â”‚  â”‚ â€¢ False Positive Rate: 0.8% (acceptable)              â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Business metrics:                                       â”‚  â”‚
â”‚  â”‚ â€¢ Fraud prevented: $8M/day                             â”‚  â”‚
â”‚  â”‚ â€¢ False declines cost: $400K/day                       â”‚  â”‚
â”‚  â”‚ â€¢ Net benefit: $7.6M/day                               â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Validation criteria (must pass ALL):                   â”‚  â”‚
â”‚  â”‚ âœ“ AUC > 0.90                                           â”‚  â”‚
â”‚  â”‚ âœ“ Recall > 0.90 (catch >90% of fraud)                â”‚  â”‚
â”‚  â”‚ âœ“ False positive rate < 1%                            â”‚  â”‚
â”‚  â”‚ âœ“ Better than current production model                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â”‚  STEP 6: DEPLOYMENT (Variable)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ If validation passes:                                   â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Phase 1: Shadow Deployment (24 hours)                  â”‚  â”‚
â”‚  â”‚ â€¢ Run alongside production                             â”‚  â”‚
â”‚  â”‚ â€¢ Log predictions, don't use them                      â”‚  â”‚
â”‚  â”‚ â€¢ Compare behavior                                      â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Phase 2: Gradual Rollout                               â”‚  â”‚
â”‚  â”‚ â€¢ 10% traffic â†’ 24 hours â†’ Monitor                    â”‚  â”‚
â”‚  â”‚ â€¢ 50% traffic â†’ 24 hours â†’ Monitor                    â”‚  â”‚
â”‚  â”‚ â€¢ 100% traffic â†’ Full deployment                       â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Rollback criteria:                                      â”‚  â”‚
â”‚  â”‚ â€¢ Performance worse than control                        â”‚  â”‚
â”‚  â”‚ â€¢ Error rate increases                                  â”‚  â”‚
â”‚  â”‚ â€¢ Business metrics degrade                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total pipeline time: ~3 hours (2 AM - 5 AM)
Frequency: Daily (ensures model stays current)
```

---

### Component 5: Decision Engine

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DECISION ENGINE                          â”‚
â”‚                                                                â”‚
â”‚  INPUT:                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ ML fraud probability: 0.72                           â”‚  â”‚
â”‚  â”‚ â€¢ Rules engine score: 0.60                             â”‚  â”‚
â”‚  â”‚ â€¢ User historical behavior: 0.45                       â”‚  â”‚
â”‚  â”‚ â€¢ Merchant risk score: 0.55                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â”‚  RISK SCORE CALCULATION:                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Weighted combination:                                   â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ risk_score = 0.5 Ã— ML_prob +                           â”‚  â”‚
â”‚  â”‚              0.3 Ã— rules_score +                        â”‚  â”‚
â”‚  â”‚              0.15 Ã— historical +                        â”‚  â”‚
â”‚  â”‚              0.05 Ã— merchant_risk                       â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ = 0.5Ã—0.72 + 0.3Ã—0.60 + 0.15Ã—0.45 + 0.05Ã—0.55         â”‚  â”‚
â”‚  â”‚ = 0.36 + 0.18 + 0.068 + 0.028                          â”‚  â”‚
â”‚  â”‚ = 0.636                                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â”‚  CONTEXT-AWARE THRESHOLD ADJUSTMENT:                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Base thresholds:                                        â”‚  â”‚
â”‚  â”‚ â€¢ < 0.3: APPROVE                                       â”‚  â”‚
â”‚  â”‚ â€¢ 0.3-0.7: REVIEW                                      â”‚  â”‚
â”‚  â”‚ â€¢ > 0.7: DECLINE                                       â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Adjustments based on context:                          â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ IF high_value_transaction (>$1000):                    â”‚  â”‚
â”‚  â”‚    threshold -= 0.1  (more conservative)               â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ IF loyal_customer (>5 years, perfect history):         â”‚  â”‚
â”‚  â”‚    threshold += 0.1  (more lenient)                    â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ IF new_account (<30 days):                             â”‚  â”‚
â”‚  â”‚    threshold -= 0.15  (more careful)                   â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ IF known_merchant (frequent purchases):                â”‚  â”‚
â”‚  â”‚    threshold += 0.05  (slightly more permissive)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â”‚  DECISION ROUTING:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Final risk score: 0.636                                â”‚  â”‚
â”‚  â”‚ Adjusted threshold: 0.3-0.7 (REVIEW zone)             â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Decision: REVIEW                                        â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Actions:                                                â”‚  â”‚
â”‚  â”‚ â€¢ Hold transaction (don't process yet)                 â”‚  â”‚
â”‚  â”‚ â€¢ Send SMS to user for confirmation                    â”‚  â”‚
â”‚  â”‚ â€¢ Request 3D Secure verification                       â”‚  â”‚
â”‚  â”‚ â€¢ Add to review queue for analyst                      â”‚  â”‚
â”‚  â”‚ â€¢ Timeout: 60 seconds (then auto-decide)              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â”‚  DISTRIBUTION:                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ APPROVE (65% of transactions)                          â”‚  â”‚
â”‚  â”‚ â€¢ Process immediately                                   â”‚  â”‚
â”‚  â”‚ â€¢ Log for monitoring                                    â”‚  â”‚
â”‚  â”‚ â€¢ Update user statistics                               â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ REVIEW (30% of transactions)                           â”‚  â”‚
â”‚  â”‚ â€¢ Additional verification steps                         â”‚  â”‚
â”‚  â”‚ â€¢ User confirmation                                     â”‚  â”‚
â”‚  â”‚ â€¢ Human review if high-risk                            â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ DECLINE (5% of transactions)                           â”‚  â”‚
â”‚  â”‚ â€¢ Block immediately                                     â”‚  â”‚
â”‚  â”‚ â€¢ Notify user                                           â”‚  â”‚
â”‚  â”‚ â€¢ Log for investigation                                 â”‚  â”‚
â”‚  â”‚ â€¢ Update fraud database                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Component 6: Monitoring & Alerting

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MONITORING DASHBOARDS                       â”‚
â”‚                                                                â”‚
â”‚  OPERATIONS DASHBOARD (ML Engineers) - Updated Hourly        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ SYSTEM HEALTH                                           â”‚  â”‚
â”‚  â”‚ â€¢ Request rate: 10,247 TPS âœ“                          â”‚  â”‚
â”‚  â”‚ â€¢ Latency P50/P95/P99: 45ms/87ms/142ms âœ“             â”‚  â”‚
â”‚  â”‚ â€¢ Error rate: 0.03% âœ“                                 â”‚  â”‚
â”‚  â”‚ â€¢ Prediction service instances: 32 active âœ“           â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ MODEL BEHAVIOR                                          â”‚  â”‚
â”‚  â”‚ â€¢ Fraud rate predicted: 0.6% âœ“                        â”‚  â”‚
â”‚  â”‚ â€¢ Approve: 65%, Review: 30%, Decline: 5% âœ“           â”‚  â”‚
â”‚  â”‚ â€¢ Prediction confidence avg: 0.82 âœ“                   â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ FEATURE HEALTH                                          â”‚  â”‚
â”‚  â”‚ â€¢ Feature cache hit rate: 98% âœ“                       â”‚  â”‚
â”‚  â”‚ â€¢ Missing feature rate: 0.2% âœ“                        â”‚  â”‚
â”‚  â”‚ â€¢ Feature computation latency: 28ms avg âœ“             â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ DATA DRIFT                                              â”‚  â”‚
â”‚  â”‚ â€¢ Feature drift (PSI): 0.08 âœ“ (< 0.1 = stable)       â”‚  â”‚
â”‚  â”‚ â€¢ Transaction amount distribution: Stable âœ“            â”‚  â”‚
â”‚  â”‚ â€¢ Geographic distribution: 2% shift âš ï¸                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â”‚  BUSINESS DASHBOARD (Stakeholders) - Updated Daily           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ BUSINESS IMPACT (Today)                                â”‚  â”‚
â”‚  â”‚ â€¢ Fraud prevented: $8.2M ðŸ’°                           â”‚  â”‚
â”‚  â”‚ â€¢ False declines cost: $380K                           â”‚  â”‚
â”‚  â”‚ â€¢ Investigation costs: $200K                            â”‚  â”‚
â”‚  â”‚ â€¢ Net benefit: $7.62M ðŸ“ˆ                              â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ MODEL PERFORMANCE                                       â”‚  â”‚
â”‚  â”‚ â€¢ Fraud catch rate: 93% âœ“ (target: >90%)             â”‚  â”‚
â”‚  â”‚ â€¢ False positive rate: 0.7% âœ“ (target: <1%)          â”‚  â”‚
â”‚  â”‚ â€¢ Customer satisfaction: 4.2/5 âœ“                      â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ INVESTIGATION QUEUE                                     â”‚  â”‚
â”‚  â”‚ â€¢ Pending reviews: 4,200 transactions                  â”‚  â”‚
â”‚  â”‚ â€¢ Avg time to resolve: 18 minutes âœ“                   â”‚  â”‚
â”‚  â”‚ â€¢ Backlog: 2.1 hours âœ“                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â”‚  ALERTING SYSTEM                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ CRITICAL (Page Immediately)                            â”‚  â”‚
â”‚  â”‚ â€¢ System down >5 min                                   â”‚  â”‚
â”‚  â”‚ â€¢ Error rate >2%                                       â”‚  â”‚
â”‚  â”‚ â€¢ Latency P95 >200ms                                   â”‚  â”‚
â”‚  â”‚ â€¢ Fraud rate >3% (spike)                               â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ HIGH (Slack + Email within 30 min)                     â”‚  â”‚
â”‚  â”‚ â€¢ Latency P95 >150ms                                   â”‚  â”‚
â”‚  â”‚ â€¢ Feature drift PSI >0.25                              â”‚  â”‚
â”‚  â”‚ â€¢ Model accuracy <90%                                  â”‚  â”‚
â”‚  â”‚ â€¢ Fraud catch rate <85%                                â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ MEDIUM (Email, investigate during hours)               â”‚  â”‚
â”‚  â”‚ â€¢ Feature drift PSI 0.1-0.25                          â”‚  â”‚
â”‚  â”‚ â€¢ Model accuracy 90-92%                                â”‚  â”‚
â”‚  â”‚ â€¢ Latency degradation trend                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Step 5: Trade-offs and Design Decisions

### Decision 1: Synchronous vs Asynchronous Processing

**Decision: Hybrid Approach**

```
Low-risk transactions (40-50%):
  â”œâ”€ Rules engine â†’ APPROVE
  â””â”€ Synchronous, <50ms
  
Medium-risk transactions (30%):
  â”œâ”€ ML prediction â†’ REVIEW
  â”œâ”€ Additional verification
  â””â”€ Semi-async, <3 seconds
  
High-risk transactions (5%):
  â”œâ”€ ML prediction â†’ DECLINE or deep investigation
  â””â”€ Can be fully async
```

**Why hybrid?**
- **Simple synchronous:** Can't handle deep analysis (too slow)
- **Fully async:** Poor user experience (long waits)
- **Hybrid:** Best of both worlds

**Trade-off:**
- **Complexity:** More complex architecture
- **Benefit:** Optimal user experience + thorough analysis

---

### Decision 2: Model Choice - XGBoost

**Considered alternatives:**

| Model | Latency | Accuracy | Interpretability | Decision |
|-------|---------|----------|------------------|----------|
| Logistic Regression | 5ms âœ“âœ“âœ“ | 82% âœ— | High âœ“âœ“âœ“ | Too simple |
| **XGBoost (CHOSEN)** | **20ms âœ“âœ“** | **94% âœ“âœ“âœ“** | **High âœ“âœ“âœ“** | **âœ“ Best balance** |
| Deep Learning | 100ms âœ— | 95% âœ“âœ“âœ“ | Low âœ— | Too slow |
| Ensemble (XGB+NN) | 120ms âœ— | 96% âœ“âœ“âœ“ | Low âœ— | Marginal gain |

**Why XGBoost wins:**
- Fast enough (20ms << 100ms budget)
- High accuracy (94% is excellent)
- Interpretable (can explain decisions for compliance)
- Proven in production (battle-tested)

**Trade-off:**
- Give up 1-2% accuracy from deep learning
- Gain 80ms latency reduction + interpretability

---

### Decision 3: Feature Store - Redis vs DynamoDB

**Decision: Redis (Online) + S3 (Offline)**

```
Comparison:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Technology  â”‚ Latency  â”‚ Throughput â”‚ Cost       â”‚ Setup  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Redis       â”‚ <10ms    â”‚ 100K/sec   â”‚ $$         â”‚ Simple â”‚
â”‚ (CHOSEN)    â”‚ âœ“âœ“âœ“      â”‚ âœ“âœ“âœ“        â”‚ âœ“âœ“         â”‚ âœ“âœ“âœ“    â”‚
â”‚             â”‚          â”‚            â”‚            â”‚        â”‚
â”‚ DynamoDB    â”‚ 10-20ms  â”‚ High       â”‚ $$$        â”‚ Simple â”‚
â”‚             â”‚ âœ“âœ“       â”‚ âœ“âœ“âœ“        â”‚ âœ—          â”‚ âœ“âœ“âœ“    â”‚
â”‚             â”‚          â”‚            â”‚            â”‚        â”‚
â”‚ Cassandra   â”‚ 10-30ms  â”‚ Very High  â”‚ $$         â”‚ Complexâ”‚
â”‚             â”‚ âœ“âœ“       â”‚ âœ“âœ“âœ“        â”‚ âœ“âœ“         â”‚ âœ—      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Redis?**
- Sub-10ms latency (critical for 100ms budget)
- Simple operational model
- 60GB dataset fits comfortably in memory
- Battle-tested for this use case

**Trade-off:**
- In-memory only (but we have backups)
- Benefit: Fastest possible feature retrieval

---

### Decision 4: Real-time vs Batch Features

**Decision: Hybrid - Pre-compute + Incremental Update**

```
BATCH (Nightly):
  Compute complex features:
  â€¢ 30-day behavior patterns
  â€¢ Merchant global statistics
  â€¢ User baseline profiles
  
  Why batch:
  â€¢ Too slow to compute in real-time
  â€¢ Need complete historical data
  
INCREMENTAL (Per Transaction):
  Update simple counters:
  â€¢ Transaction count last 24h
  â€¢ Total spend last 24h
  â€¢ Recent merchants set
  
  Why incremental:
  â€¢ Must be current (1-minute old data acceptable)
  â€¢ Simple arithmetic (fast)
```

**Alternatives considered:**
- **All real-time:** Can't compute 90-day aggregates in 100ms
- **All batch:** Misses recent fraud patterns (stale)
- **Hybrid (chosen):** Best of both

---

### Decision 5: Geographic Architecture

**Decision: Multi-region Active-Active**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          MULTI-REGION DEPLOYMENT                          â”‚
â”‚                                                            â”‚
â”‚  US-EAST            US-WEST           EU-WEST     ASIA    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚Predictionâ”‚       â”‚Predictionâ”‚      â”‚Predictionâ”‚ â”‚Pred.â”‚ â”‚
â”‚  â”‚Services â”‚       â”‚Services â”‚      â”‚Services â”‚ â”‚Serv.â”‚ â”‚
â”‚  â”‚         â”‚       â”‚         â”‚      â”‚         â”‚ â”‚     â”‚ â”‚
â”‚  â”‚Feature  â”‚       â”‚Feature  â”‚      â”‚Feature  â”‚ â”‚Feat.â”‚ â”‚
â”‚  â”‚Store    â”‚       â”‚Store    â”‚      â”‚Store    â”‚ â”‚Storeâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â”‚
â”‚       â”‚                 â”‚                â”‚         â”‚    â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                 â”‚
â”‚                         â–¼                                 â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚  Shared Model        â”‚                    â”‚
â”‚              â”‚  Registry (S3)       â”‚                    â”‚
â”‚              â”‚  Cross-region repl.  â”‚                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why multi-region?**
- **Latency:** Local prediction reduces network time
- **Reliability:** Region failure doesn't take down system
- **Compliance:** Data residency requirements (GDPR)

**Alternative considered:**
- **Single region:** Latency too high for global users (150-300ms)
- **Active-passive:** Wastes capacity, slower failover

**Trade-off:**
- **Cost:** 4x infrastructure
- **Benefit:** Global <50ms latency, 99.99% availability

---

## Step 6: Scaling and Bottlenecks

### Scaling to 50,000 TPS (5x current)

**What needs to change:**

**1. Prediction Services:**
```
Current: 30 instances (333 TPS each)
Future: 150 instances (333 TPS each)

Auto-scaling handles this automatically:
â€¢ Horizontal scaling (add more instances)
â€¢ No code changes needed
â€¢ Cost: 5x
```

**2. Feature Store (Redis):**
```
Current: 5-node cluster
  â€¢ 60GB data / 5 nodes = 12GB per node
  â€¢ 10,000 TPS / 5 nodes = 2,000 TPS per node
  
Future: 15-node cluster
  â€¢ 60GB data / 15 nodes = 4GB per node
  â€¢ 50,000 TPS / 15 nodes = 3,333 TPS per node
  
Still well within Redis capacity (100K+ ops/sec)
```

**3. Load Balancers:**
```
Current: 2 load balancers
Future: 6 load balancers (geographic distribution)
```

**4. Cost:**
```
Current: $50K/month
Future: $150K/month (3x traffic, slightly less than 3x cost)

Why not 5x cost?
â€¢ Economies of scale
â€¢ Same batch pipeline
â€¢ Same model registry
```

### Bottleneck Analysis

**Bottleneck 1: Feature Computation (30ms)**

**Problem:** Feature extraction takes 30ms of our 100ms budget

**Solutions implemented:**
1. **Parallel fetches:** Transaction + User + Aggregates simultaneously (10ms improvement)
2. **Redis pipelining:** Batch multiple GET commands (5ms improvement)
3. **Feature caching:** Cache computed features for 60 seconds (5ms improvement)
4. **Result:** 30ms â†’ 15ms

---

**Bottleneck 2: Peak Traffic (50K TPS)**

**Problem:** Black Friday 5x spike

**Solutions:**
1. **Auto-scaling:** Scale to 150 instances in <2 minutes
2. **Request queuing:** Buffer bursts (max queue: 60 seconds)
3. **Rules engine bypass:** Handles 50% of peak traffic without ML
4. **Predictive scaling:** Pre-scale before known events

---

**Bottleneck 3: Feature Store Memory (60GB)**

**Problem:** All features must fit in memory

**Solutions:**
1. **Sharding:** Distribute across multiple nodes (current: 5, future: 15)
2. **TTL policy:** Expire old data (aggregates >30 days)
3. **Tiering:** Hot data in Redis, warm data in cache, cold data on-demand
4. **Currently:** Not a bottleneck (60GB << modern server capacity)

---

**Bottleneck 4: Model Staleness**

**Problem:** Patterns change, model degrades over time

**Solutions:**
1. **Daily retraining:** Fresh patterns every day
2. **Drift detection:** Triggers early retrain if needed
3. **A/B testing:** Safe deployment of new models
4. **Online learning (future):** Continuous adaptation

---

## Step 7: Failure Scenarios and Resilience

### Scenario 1: Redis Feature Store Failure

```
Problem: Redis master node fails
Impact: Can't retrieve user features (30% of feature data)

Resilience:
1. Automatic failover to replica (<5 seconds)
2. All reads continue from other nodes (degraded but functional)
3. Writes queue until master restored
4. Fallback: Use default/cached features for critical transactions

Recovery Time: 5-30 seconds
User Impact: Minimal (brief latency spike)
```

### Scenario 2: Model Prediction Service Failure

```
Problem: 50% of prediction instances crash

Resilience:
1. Load balancer detects unhealthy instances immediately
2. Routes traffic to healthy instances
3. Auto-scaling triggers (adds more instances)
4. Temporary overload on healthy instances (latency increases)

Fallback:
â€¢ Rules engine handles >50% of traffic
â€¢ Queue low-priority requests
â€¢ Increase review threshold (more human review, less auto-decline)

Recovery Time: 2-5 minutes (auto-scaling)
User Impact: Moderate (increased latency, more reviews)
```

### Scenario 3: Complete Region Failure

```
Problem: US-East region goes down

Resilience:
1. DNS failover routes US traffic to US-West (<30 seconds)
2. US-West scales up to handle additional load
3. Feature stores sync'd across regions (slight staleness acceptable)
4. Models available in all regions

Recovery Time: 30 seconds - 5 minutes
User Impact: 30-60 second outage, then normal operation
```

### Scenario 4: Model Performance Degradation

```
Problem: Model accuracy drops from 94% to 80% (data drift)

Detection:
â€¢ Drift monitoring alerts (PSI > 0.25)
â€¢ Performance metrics show decline
â€¢ Business metrics affected

Response:
1. Immediate: Adjust decision thresholds (compensate)
2. Short-term: Trigger emergency retraining (4 hours)
3. Long-term: Investigate root cause, fix data pipeline

User Impact: Gradual degradation over days, caught before severe
```

---

## Interview Answer Framework

**"For a real-time fraud detection system at 10K TPS with <100ms latency, I'd design:**

### High-Level Architecture
- **API Gateway â†’ Rules Engine (5ms) â†’ Feature Extraction (30ms) â†’ ML Prediction (20ms) â†’ Decision Logic (5ms)**
- **Total: 60ms P50, 90ms P95** (within budget)

### Key Components

**1. Rules Engine:**
- Handles 40-50% of transactions without ML
- Hard rules (auto-decline) + soft rules (auto-approve)
- Reduces computational load

**2. Feature Store:**
- Redis cluster for <10ms feature lookups
- Pre-computed aggregates (24h, 7d, 30d windows)
- Incremental updates per transaction
- 60GB data, 5-node cluster

**3. ML Model:**
- XGBoost for fast inference (20ms) and interpretability
- Why not deep learning: Too slow (100ms+)
- 50 features: transaction, user, aggregates, derived
- 100MB model loaded in memory

**4. Decision Engine:**
- Risk score combines ML + rules + history
- Context-aware thresholds (adjust by amount, user, merchant)
- Routes to: Approve (65%), Review (30%), Decline (5%)

**5. Batch Pipeline:**
- Daily retraining at 2 AM
- Handles class imbalance (0.5% fraud rate)
- Shadow deployment + gradual rollout
- Continuous monitoring for drift

### Scaling Strategy
- **Horizontal scaling:** 30 instances â†’ 150 for 5x traffic
- **Sharded feature store:** 5 â†’ 15 node cluster
- **Multi-region deployment:** US, EU, Asia
- **Auto-scaling:** Handles traffic spikes

### As a Data Engineer, I'd emphasize:
- **Design for scalability from day one** (stateless services, sharded data)
- **Monitor extensively** (4 layers: infrastructure, behavior, performance, business)
- **Automate retraining** (daily) and deployment (gradual rollout)
- **Handle the 0.5% fraud rate** class imbalance carefully

### Key Trade-offs
1. **XGBoost over deep learning:** Speed over marginal accuracy (20ms vs 100ms)
2. **Hybrid sync/async:** User experience over simplicity
3. **Pre-compute + increment:** Latency over absolute freshness
4. **Multi-region active-active:** Latency/reliability over cost

**The system handles 10K TPS today, scales to 50K TPS with no architecture changes, and maintains 94% fraud detection while keeping false positives under 1%."**

---

**END OF Q9**




